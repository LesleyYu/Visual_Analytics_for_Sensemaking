{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Keywords extraction\n",
    "\n",
    "> ## Content\n",
    ">\n",
    "> Pre-processing user data (by web-scraping)\n",
    ">\n",
    "> Keywords Extraction\n",
    ">\n",
    "> 1. RAKE (Rapid Automatic Keyword Extraction algorithm)\n",
    "> 2. YAKE (Yet Another Keyword Extractor)\n",
    "> 3. Postion Rank (pke unsupervised model)\n",
    "> 4. Topic Rank (pke unsupervised model)\n",
    "> 5. Multi-partite Rank (pke unsupervised model)\n",
    "> 6. KeyBERT\n",
    ">\n",
    "> Method Evaluation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lesley/opt/anaconda3/envs/IDS/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "import urllib\n",
    "from html.parser import HTMLParser\n",
    "import json\n",
    "import re   # get rid of weird white spaces\n",
    "\n",
    "# for keywords extraction \n",
    "from keybert import KeyBERT\n",
    "from rake_nltk import Rake\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "import yake\n",
    "import pke\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "\n",
    "# for method evaluation\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing user data\n",
    "\n",
    "Read all the URLs from user data, then do some web-scraping to get every sentence in the webpages user has browsed.\n",
    "\n",
    "Data is stored in `sentences` for further natural language processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words parser class\n",
    "class WordsParser(HTMLParser):\n",
    "    # tags to search text within\n",
    "    search_tags = ['p', 'div', 'span', 'a']\n",
    "    \n",
    "    # current tag\n",
    "    current_tag = ''\n",
    "    \n",
    "    # list of all sentences\n",
    "    all_words = []\n",
    "    \n",
    "    # handle starting tag\n",
    "    def handle_starttag(self, tag, attr):\n",
    "        # store current tag\n",
    "        self.current_tag = tag        \n",
    "            \n",
    "    # handle tag's data\n",
    "    def handle_data(self, data):\n",
    "        \n",
    "      # make sure current tag matches search tags\n",
    "      if self.current_tag in self.search_tags:\n",
    "        if (\n",
    "            (('.' in data) or ('!' in data) or ('?' in data)) and\n",
    "            ('...' not in data)\n",
    "        ):\n",
    "            \n",
    "            # clean the data\n",
    "            data = re.sub('\\s+' ,' ', data)\n",
    "            \" \".join(data.split())      # remove duplicate spaces and newline characters\n",
    "            \n",
    "            # add to the list\n",
    "            self.all_words.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=umich+biostatistics+master&oq=umich+biostatistics+master&aqs=chrome..69i57j69i60l3.11673j0j7&sourceid=chrome&ie=UTF-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/1f1h3f353951kvdp0mtpgndw0000gn/T/ipykernel_61815/405886381.py:31: DeprecationWarning: AppURLopener style of invoking requests is deprecated. Use newer urlopen functions/methods\n",
      "  opener = AppURLopener()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100632\n",
      "https://sph.umich.edu/biostat/\n",
      "156026\n",
      "https://sph.umich.edu/biostat/programs/masters.html\n",
      "196971\n",
      "https://sph.umich.edu/biostat/apply-ms-biostatistics.html\n",
      "243121\n",
      "https://rackham.umich.edu/admissions/applying/\n",
      "467909\n"
     ]
    }
   ],
   "source": [
    "# main driver\n",
    "if __name__ == '__main__':\n",
    "  # read all json files to get URLs\n",
    "  URLs = []\n",
    "  cnt = 0\n",
    "  for i in range(1):                # <------- change no. of user data files here\n",
    "      with open('../data/{}.json'.format(i+1)) as openfile:\n",
    "\n",
    "          json_object = json.load(openfile)\n",
    "\n",
    "          # # the num of webpages every json file visitd\n",
    "          # numPages = len(json_object[\"data\"])\n",
    "          # cnt = numPages + cnt\n",
    "\n",
    "          # append all urls into a list `URLs`\n",
    "          # for j in range(len(json_object[\"data\"])):\n",
    "          for j in range(5):        # <------- change no. of urls here\n",
    "              URLs.append(json_object[\"data\"][j]['url'])\n",
    "\n",
    "  # # if (cnt == len(URLs)): print(\"true\")        # check if all webpages are put into `URLs`\n",
    "\n",
    "  html = \"\"\n",
    "\n",
    "  # target URL to scrape\n",
    "  for url in URLs:\n",
    "      print(url)\n",
    "      # make HTTP GET request to the target URL\n",
    "      class AppURLopener(urllib.request.FancyURLopener):\n",
    "          version = \"Mozilla/5.0\"\n",
    "\n",
    "      opener = AppURLopener()\n",
    "      response = opener.open(url)\n",
    "\n",
    "      # extract HTML document from response\n",
    "      html = response.read().decode('utf-8', errors='ignore') + html\n",
    "      print(len(html))\n",
    "\n",
    "  # create words parser instance\n",
    "  words_parser = WordsParser()\n",
    "\n",
    "  # feed HTML to words parser\n",
    "  words_parser.feed(html)\n",
    "\n",
    "  # get all full sentences\n",
    "  sentences = words_parser.all_words\n",
    "\n",
    "  # now that we have got all the sentences webpages user browsed\n",
    "  # save to a txt file for backup\n",
    "  # print(sentences)\n",
    "  with open(\"../web_scraping/sentences.txt\", \"w\") as f:\n",
    "      for sentence in sentences:\n",
    "          f.write(sentence + '\\n')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords Extraction\n",
    "\n",
    "https://towardsdatascience.com/keyword-extraction-a-benchmark-of-7-algorithms-in-python-8a905326d93f\n",
    "\n",
    "method 2, 3, 4, 5: https://boudinfl.github.io/pke/build/html/unsupervised.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small dataset for quick test, can delete later\n",
    "# sentences = ['Why Study Public Health?', 'Graduate programs in the Department of Biostatistics at the University of Michigan School of Public Health are among the best in the world. Currently, we are ranked as the No. 4 biostatistics department by US News and World Report. ']\n",
    "\n",
    "# initiate BERT outside of functions\n",
    "bert = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'am', 'has', 'three', 'around', 'front', 'seemed', 'again', 'himself', 'back', 'would', 'mostly', 'with', 'always', 'besides', 'yourselves', 'off', 'beyond', 'have', 'does', \"'re\", 'anyway', 'take', 'themselves', 'full', 'ten', '’m', 'whom', 'noone', 'against', 'therefore', 'because', 'therein', 'meanwhile', 'moreover', 'per', 'mine', 'amount', 'and', 'never', 'each', 'what', 'over', 'four', 'although', 'already', '‘m', 'they', 'towards', 'across', \"'ll\", 'hereupon', 'by', 'be', 'in', 'eight', 'almost', 'top', 'nowhere', 'or', 'unless', 'others', 'n‘t', 'that', 'been', 'now', 'until', 'another', 'i', 'once', 'became', 'whoever', 'go', 'whereupon', 'ever', 'whole', 'down', 'along', 'third', 'whatever', 'were', 'something', 'name', 'all', 'whose', 'really', 'except', 'thereupon', 'their', 'some', 'below', 'call', 'nor', 'also', 'herself', 'next', 'while', 'your', 'everything', 'often', 'such', 'indeed', 'sometime', 'should', 'eleven', 'may', 'sixty', 'whereby', 'less', 'within', 'are', 'thus', '’re', 'empty', 'regarding', 'but', 'why', 'fifteen', 'out', 'done', 'hereafter', 'must', 'no', 'well', 'only', 'between', 'these', 'anyone', 'due', 'upon', 'not', 'whereas', 'using', 'me', '‘s', 'somehow', 'alone', 'make', 'someone', 'become', 'her', 'my', 'yours', 'every', 'forty', 're', 'anywhere', 'any', 'under', 'both', 'none', 'one', 'even', 'thereafter', 'thru', 'however', 'serious', 'used', 'he', '’s', 'so', 'is', 'side', 'as', 'five', 'might', 'please', \"'s\", 'own', '‘d', 'throughout', 'before', 'least', 'on', 'through', 'quite', 'will', 'anything', 'otherwise', 'us', 'you', 'fifty', 'him', 'thereby', 'then', 'six', \"'d\", 'nothing', 'his', 'thence', 'hundred', 'from', 'everywhere', 'onto', 'give', 'itself', 'here', 'nobody', 'last', 'toward', 'who', 'several', 'yet', \"'m\", 'sometimes', 'enough', 'when', 'amongst', 'seeming', 'same', 'somewhere', 'at', \"'ve\", 'into', 'whither', 'rather', 'see', 'an', 'had', 'of', 'other', 'among', 'she', 'whether', 'the', 'n’t', 'just', 'do', 'for', 'move', 'hereby', 'there', 'still', 'if', 'seems', 'a', 'since', 'else', 'cannot', 'much', 'nevertheless', 'ours', 'perhaps', 'many', 'say', 'could', 'formerly', 'hers', 'too', 'our', 'about', 'it', 'whenever', 'elsewhere', 'put', 'anyhow', 'via', 'doing', 'twenty', 'though', 'more', 'beforehand', 'which', 'to', 'after', 'was', '’ll', 'becoming', 'where', 'twelve', 'whence', 'beside', '‘ve', 'whereafter', 'first', 'without', 'bottom', 'everyone', 'wherever', 'hence', 'those', '‘ll', 'seem', 'myself', 'becomes', 'than', 'this', '’d', 'behind', 'latter', 'show', 'up', 'afterwards', 'above', 'we', 'yourself', 'herein', 'keep', 'made', 'did', 'namely', 'part', 'two', 'most', 'either', 'how', 'together', 'can', 'former', 'latterly', 'them', '‘re', 'get', 'very', '’ve', 'few', 'its', \"n't\", 'ourselves', 'nine', 'being', 'further', 'wherein', 'during', 'ca', 'various', 'neither'}\n"
     ]
    }
   ],
   "source": [
    "# initialize stop-wrod list\n",
    "print(pke.lang.stopwords.get('en'))\n",
    "\n",
    "stoplist = list(string.punctuation)         # punctuations:  !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "stoplist += pke.lang.stopwords.get('en')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forms of data for further processing\n",
    "\n",
    "**sentences**: list of strings\n",
    "\n",
    "**article**: string\n",
    "\n",
    "**doc**: spacy model processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n",
      "Why Study Public Health? Graduate programs in the Department of Biostatistics at the University of Michigan School of Public Health are among the best in the world. Currently, we are ranked as the No. 4 biostatistics department by US News and World Report.  \n"
     ]
    }
   ],
   "source": [
    "# spacy models\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "print([(w.text, w.pos_) for w in doc])\n",
    "\n",
    "article = ''\n",
    "for sentence in sentences:\n",
    "    sentence +=  \" \"\n",
    "    article += sentence\n",
    "doc = nlp(article)\n",
    "\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher \n",
    "# checks if a list of keywords match a certain POS pattern\n",
    "def match(text):\n",
    "    patterns = [\n",
    "        [{'POS': 'PROPN'}, {'POS': 'VERB'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}],  \n",
    "        [{'POS': 'NOUN'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'ADV'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'ADP'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'ADP'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'NOUN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'VERB'}, {'POS': 'ADV'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'NOUN'}],\n",
    "        [{'POS': 'VERB'}],\n",
    "        [{'POS': 'ADJ'}],\n",
    "        ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"pos-matcher\", patterns)\n",
    "    # create spacy object\n",
    "    doc = nlp(text)\n",
    "    # iterate through the matches\n",
    "    matches = matcher(doc)\n",
    "    # if matches is not empty, it means that it has found at least a match\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "match(\"very delicious cake\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RAKE\n",
    "\n",
    "https://pypi.org/project/rake-nltk/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edu › biostat › programs › mastersthe ms': 43, 'edu › biostat › programs › masters': 38, 'edu › biostat › programssph': 28}\n"
     ]
    }
   ],
   "source": [
    "def rake_extractor(text, n):\n",
    "    # Uses stopwords for english from NLTK, and all puntuation characters by default\n",
    "    r = Rake()\n",
    "\n",
    "    # Extraction given string\n",
    "    # r.extract_keywords_from_text(text)    <-- raises error\n",
    "    \n",
    "    # Extraction given the list of strings where each string is a sentence.\n",
    "    r.extract_keywords_from_sentences([text])\n",
    "\n",
    "    # To get keyword phrases ranked highest to lowest with scores.\n",
    "    keywords_scored = r.get_ranked_phrases_with_scores()[:n]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for keyword in keywords_scored:\n",
    "        # The lower the score, the more relevant the keyword is.\n",
    "        # print(\"keyword \\'\", keyword[1], \"\\' scores\", keyword[0])\n",
    "\n",
    "        # convert scores to integers\n",
    "        score = int(keyword[0])\n",
    "        results[keyword[1]] = score\n",
    "\n",
    "    return results \n",
    "    \n",
    "result_rake = rake_extractor(article, 5)\n",
    "print(result_rake)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. YAKE\n",
    "\n",
    "https://liaad.github.io/yake/docs/getting_started\n",
    "\n",
    "https://github.com/LIAAD/yake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Graduate': 93, 'Rackham': 90, 'programs': 89, 'Biostatistics': 88, 'program': 81}\n"
     ]
    }
   ],
   "source": [
    "def yake_extractor(text, n):\n",
    "\n",
    "    # params tuning\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 1\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 1\n",
    "    numOfKeywords = n\n",
    "    \n",
    "    scored_keywords = yake.KeywordExtractor(lan=language, \n",
    "                                     n=max_ngram_size, \n",
    "                                     dedupLim=deduplication_thresold, \n",
    "                                     dedupFunc=deduplication_algo, \n",
    "                                     windowsSize=windowSize, \n",
    "                                     top=numOfKeywords).extract_keywords(text)\n",
    "    \n",
    "    results = {}\n",
    "    for keyword in scored_keywords:\n",
    "        # The lower the score, the more relevant the keyword is.\n",
    "        # print(\"keyword \\'\", keyword[0], \"\\' scores\", keyword[1])\n",
    "\n",
    "        # convert scores to integers\n",
    "        score = 100 - int(keyword[1] * 1000)\n",
    "        results[keyword[0]] = score\n",
    "    return results\n",
    "\n",
    "# usage\n",
    "article = ''\n",
    "for sentence in sentences:\n",
    "    article += sentence\n",
    "\n",
    "result_yake = yake_extractor(article, 5)\n",
    "print(result_yake)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PositionRank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'graduate': 104, 'program': 63, 'rackham': 42, 'schools': 39, 'michigan': 32}\n"
     ]
    }
   ],
   "source": [
    "def position_rank_extractor(text, n):\n",
    "\n",
    "    # define the valid Part-of-Speeches to occur in the graph\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}      # , 'ADV'\n",
    "\n",
    "    # define extractor & load document\n",
    "    extractor = pke.unsupervised.PositionRank()\n",
    "    extractor.load_document(text, language='en')\n",
    "\n",
    "    # candidate number\n",
    "    extractor.candidate_selection(maximum_word_number=1)\n",
    "\n",
    "    # weight the candidates using the sum of their word's scores that are computed using random walk biaised with the position of the words in the document. In the graph, nodes are words (nouns and adjectives only) that are connected if they occur in a window of 3 words.\n",
    "    # window:  the window within the sentence for connecting two words in the graph, defaults to 10.\n",
    "    extractor.candidate_weighting(window=3, pos=pos)\n",
    "    \n",
    "    # get the 5-highest scored candidates as keyphrases\n",
    "    scored_keywords = extractor.get_n_best(n=n)\n",
    "    \n",
    "    results = {}\n",
    "    for keyword in scored_keywords:\n",
    "        # The lower the score, the more relevant the keyword is.\n",
    "        # print(\"keyword \\'\", keyword[0], \"\\' scores\", keyword[1])\n",
    "\n",
    "        # convert scores to integers\n",
    "        score = int(keyword[1] * 1000)\n",
    "        results[keyword[0]] = score\n",
    "    return results \n",
    "\n",
    "# usage\n",
    "result_position_rank = position_rank_extractor(doc, 5)\n",
    "print(result_position_rank)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TopicRank\n",
    "\n",
    "Uses TopicRank to extract the top 5 keywords from a text\n",
    "\n",
    "Arguments: text (str)\n",
    "\n",
    "Returns: list of keywords (list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'application review process': 4, 'graduate education': 3, 'graduate students': 3, 'biostatistics': 3, 'master': 3}\n"
     ]
    }
   ],
   "source": [
    "# method tuning\n",
    "def topic_rank_extractor(text, n):\n",
    "\n",
    "    # use TopicRank as extractor\n",
    "    extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "    # load content with stopwords and punctuations\n",
    "    extractor.load_document(text, language='en', stoplist=stoplist)\n",
    "\n",
    "    # sentence structure pattern\n",
    "    # select the longest sequences of nouns and adjectives, that do not contain punctuation marks or stopwords as candidates.\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}   #, 'ADV'\n",
    "    extractor.candidate_selection(pos=pos)\n",
    "\n",
    "    # default:\n",
    "    # build topics by grouping candidates with HAC (average linkage, threshold of 1/4 of shared stems). \n",
    "    # Weight the topics using random walk, and select the first occuring candidate from each topic.\n",
    "    extractor.candidate_weighting()\n",
    "\n",
    "    # get the 5-highest scored candidates as keyphrases\n",
    "    scored_keywords = extractor.get_n_best(n=n)\n",
    "    \n",
    "    results = {}\n",
    "    for keyword in scored_keywords:\n",
    "        # The lower the score, the more relevant the keyword is.\n",
    "        # print(\"keyword \\'\", keyword[0], \"\\' scores\", keyword[1])\n",
    "\n",
    "        # convert scores to integers\n",
    "        score = int(keyword[1] * 100)\n",
    "        results[keyword[0]] = score\n",
    "    return results \n",
    "\n",
    "# # usage\n",
    "result_topic_rank = topic_rank_extractor(doc, 5)\n",
    "print(result_topic_rank)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. MultipartiteRank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'graduate education': 6, 'application review process': 3, 'graduate students': 2, 'rackham graduate school': 2, 'master': 2}\n"
     ]
    }
   ],
   "source": [
    "def multipartite_rank_extractor(text, n):\n",
    "    \n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "    extractor.load_document(text, language='en', stoplist=stoplist)\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}\n",
    "    extractor.candidate_selection(pos=pos)\n",
    "\n",
    "    # build the Multipartite graph and rank candidates using random walk, alpha controls the weight adjustment mechanism\n",
    "    extractor.candidate_weighting(alpha=1.1, threshold=0.74, method='average')\n",
    "    \n",
    "    # get the 5-highest scored candidates as keyphrases\n",
    "    scored_keywords = extractor.get_n_best(n=n)\n",
    "    \n",
    "    results = {}\n",
    "    for keyword in scored_keywords:\n",
    "        # The lower the score, the more relevant the keyword is.\n",
    "        # print(\"keyword \\'\", keyword[0], \"\\' scores\", keyword[1])\n",
    "\n",
    "        # convert scores to integers\n",
    "        score = int(keyword[1] * 100)\n",
    "        results[keyword[0]] = score\n",
    "    return results \n",
    "\n",
    "# # usage\n",
    "result_mtpartite_rank = multipartite_rank_extractor(doc, 5)\n",
    "print(result_mtpartite_rank)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. KeyBERT\n",
    "\n",
    "https://towardsdatascience.com/how-to-extract-relevant-keywords-with-keybert-6e7b3cf889ae\n",
    "\n",
    "https://maartengr.github.io/KeyBERT/guides/quickstart.html#usage\n",
    "\n",
    "Uses KeyBERT to extract the top 5 one-word-long keywords from a text, combined with _frequency count_\n",
    "\n",
    "Arguments(str): text\n",
    "\n",
    "Returns(dictrionary): list of keywords, with scores add up together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'graduate': 48, 'graduates': 44, 'rackham': 42, 'scholarship': 41, 'doctoral': 39}\n"
     ]
    }
   ],
   "source": [
    "def keybert_extractor(text, n):\n",
    "    keywords = bert.extract_keywords(\n",
    "                    text, \n",
    "                    keyphrase_ngram_range=(1, 1), \n",
    "                    stop_words= 'english', \n",
    "                    top_n=n,\n",
    "                    diversity = 0.8)\n",
    "\n",
    "    results = {}\n",
    "    #index = 0\n",
    "\n",
    "    # get keywords in every sentence\n",
    "    # for scored_keywords in keywords:\n",
    "\n",
    "        # index = index + 1\n",
    "        # print(\"keywords in sentence\", index, \"is\", scored_keywords)\n",
    "\n",
    "        # loop every keyword in each sentence\n",
    "    for keyword in keywords:\n",
    "        word = keyword[0]\n",
    "        score = int(float(keyword[1] * 100) )\n",
    "        try:\n",
    "            # try to update count of the given keyword if available\n",
    "            results[word] += score\n",
    "            # print(\"update:\", word, score)\n",
    "        \n",
    "        except:\n",
    "            # store current keyword\n",
    "            results[word] = score\n",
    "            # print(\"record:\", word)\n",
    "    \n",
    "    # get top n keywords\n",
    "    import operator\n",
    "    import itertools\n",
    "    sorted_results = dict( sorted(results.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    results = dict(itertools.islice(sorted_results.items(), n))\n",
    "    return results \n",
    "\n",
    "# # usage:\n",
    "result_KeyBERT = keybert_extractor(article, 5)\n",
    "print(result_KeyBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 .\n",
      "('edu › biostat › programs › masters', 37)\n",
      "('edu › biostat › programs sph', 28)\n",
      "2 .\n",
      "('Graduate', 93)\n",
      "('Rackham', 90)\n",
      "('programs', 89)\n",
      "('Biostatistics', 88)\n",
      "('program', 81)\n",
      "3 .\n",
      "('graduate', 104)\n",
      "('program', 63)\n",
      "('rackham', 42)\n",
      "('schools', 39)\n",
      "('michigan', 32)\n",
      "4 .\n",
      "('application review process', 4)\n",
      "('graduate education', 3)\n",
      "('graduate students', 3)\n",
      "('biostatistics', 3)\n",
      "('master', 3)\n",
      "5 .\n",
      "('graduate education', 6)\n",
      "('application review process', 3)\n",
      "('graduate students', 2)\n",
      "('rackham graduate school', 2)\n",
      "('master', 2)\n",
      "6 .\n",
      "('graduate', 48)\n",
      "('graduates', 44)\n",
      "('rackham', 42)\n",
      "('scholarship', 41)\n",
      "('doctoral', 39)\n"
     ]
    }
   ],
   "source": [
    "# outputs\n",
    "results = [result_rake, result_yake, result_position_rank, result_topic_rank, result_mtpartite_rank, result_KeyBERT]\n",
    "i = 1\n",
    "\n",
    "# score normalization\n",
    "# ... code here ...\n",
    "\n",
    "for result in results:\n",
    "    print(i, \".\")\n",
    "    for item in result.items():\n",
    "      print(item)\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get seconds from time.\n",
    "def get_sec(time_str):\n",
    "    h, m, s = time_str.split(':')\n",
    "    return int(h) * 3600 + int(m) * 60 + int(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses an extractor to retrieve keywords from a list of documents\n",
    "def extract_keywords_from_corpus(extractor, corpus, n):\n",
    "    extractor_name = extractor.__name__.replace(\"_extractor\", \"\")\n",
    "    logging.info(f\"Starting keyword extraction with {extractor_name}\")\n",
    "    corpus_kws = {}\n",
    "    start = time.time()\n",
    "    # logging.info(f\"Timer initiated.\")     # output start of timer\n",
    "    for idx, text in tqdm.tqdm(enumerate(corpus), desc=\"Extracting keywords from corpus...\"):\n",
    "        corpus_kws[idx] = extractor(text, n)\n",
    "    end = time.time()\n",
    "    # logging.info(f\"Timer stopped.\")       # output end of timer\n",
    "    elapsed = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
    "    logging.info(f\"Time elapsed: {elapsed}\")\n",
    "    \n",
    "    return {\"algorithm\": extractor.__name__, \n",
    "            \"corpus_kws\": corpus_kws, \n",
    "            \"elapsed_time\": elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function runs the benchmark for the above keyword extraction algorithms\n",
    "def benchmark(corpus, shuffle=True):\n",
    "    logging.info(\"Starting benchmark...\\n\")\n",
    "    \n",
    "    # Shuffle the corpus\n",
    "    if shuffle:\n",
    "        random.shuffle(corpus)\n",
    "\n",
    "    # extract keywords from corpus\n",
    "    results = []\n",
    "    extractors = [\n",
    "        rake_extractor, \n",
    "        yake_extractor, \n",
    "        topic_rank_extractor, \n",
    "        position_rank_extractor,\n",
    "        multipartite_rank_extractor,\n",
    "        keybert_extractor,\n",
    "    ]\n",
    "    for extractor in extractors:\n",
    "        result = extract_keywords_from_corpus(extractor, corpus, 8) # <-- change num of keywrods here\n",
    "        results.append(result)\n",
    "\n",
    "    # compute average number of extracted keywords\n",
    "    for result in results:\n",
    "        len_of_kw_list = []\n",
    "        for kws in result[\"corpus_kws\"].values():\n",
    "            len_of_kw_list.append(len(kws))\n",
    "        result[\"avg_keywords_per_document\"] = np.mean(len_of_kw_list)\n",
    "\n",
    "    # match keywords\n",
    "    for result in results:\n",
    "        for idx, kws in result[\"corpus_kws\"].items():\n",
    "            match_results = []\n",
    "            for kw in kws:\n",
    "                match_results.append(match(kw))\n",
    "                result[\"corpus_kws\"][idx] = match_results\n",
    "\n",
    "    # compute average number of matched keywords\n",
    "    for result in results:\n",
    "        len_of_matching_kws_list = []\n",
    "        for idx, kws in result[\"corpus_kws\"].items():\n",
    "            len_of_matching_kws_list.append(len([kw for kw in kws if kw]))\n",
    "        result[\"avg_matched_keywords_per_document\"] = np.mean(len_of_matching_kws_list)\n",
    "        # compute average percentange of matching keywords, round 2 decimals\n",
    "        result[\"avg_percentage_matched_keywords\"] = round(\n",
    "            result[\"avg_matched_keywords_per_document\"] / result[\"avg_keywords_per_document\"], \n",
    "            2)\n",
    "        \n",
    "    # create score based on the avg percentage of matched keywords divided by time elapsed (in seconds)\n",
    "    for result in results:\n",
    "        elapsed_seconds = get_sec(result[\"elapsed_time\"]) + 0.1\n",
    "        # weigh the score based on the time elapsed\n",
    "        result[\"performance_score\"] = round(result[\"avg_percentage_matched_keywords\"] / elapsed_seconds, 2)\n",
    "    \n",
    "    # # delete corpus_kw\n",
    "    # for result in results:\n",
    "    #     del result[\"corpus_kws\"]\n",
    "\n",
    "    # create results dataframe\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"method_eval.csv\", index=False)\n",
    "    logging.info(\"Benchmark finished. Results saved to results.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting keywords from corpus...: 70it [00:00, 4643.68it/s]\n",
      "Extracting keywords from corpus...: 70it [00:00, 277.68it/s]\n",
      "Extracting keywords from corpus...: 70it [00:49,  1.42it/s]\n",
      "Extracting keywords from corpus...: 70it [00:49,  1.41it/s]\n",
      "Extracting keywords from corpus...: 70it [00:51,  1.37it/s]\n",
      "Extracting keywords from corpus...: 70it [00:04, 16.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "results = benchmark(sentences[:500], shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Output for Word-cloud Viuslisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graduate education at the University of Michigan is a shared enterprise. The Rackham Graduate School works together with faculty in the schools and colleges of the University to provide more than 180 graduate degree programs and to sustain a dynamic intellectual climate within which graduate students thrive.The Rackham Graduate School and the graduate program work as a team to manage the application review process. As an applicant you will be interacting with both offices.The University of Michigan provides many sources of financial assistance to help students meet educational and living expenses. Whether you are a prospective student, a current student, a master’s or doctoral student, we want to make sure you know about the funding available for your graduate education.From your first registration through the final stages of the degree process, we’re here to help every step of the way.Ph.D.D.M.A.Rackham offers opportunities, funding, and resources that prepare graduate students and postdoctoral fellows to access the wide range of careers available to them. Our Rackham professional development and engagement programs – from our DEI Certificate to our Program in Public Scholarship – provide outlets for interdisciplinary learning and community building related to professional interests.Resources to support the diverse backgrounds and experiences that compose our Rackham community.Over 8,300 students are enrolled in Rackham degree programs taught and advised by faculty in graduate programs situated within 18 of the 19 schools and colleges across the Ann Arbor campus. Another 7,000 students are enrolled in graduate and professional degree programs administered separately by individual schools and colleges at U-M.Find Rackham and university information and resources related to the evolving situation with COVID-19.The Rackham Graduate School and the graduate program work as a team to manage the application review process. As an applicant you will be interacting with both offices.Applicants must first consult the program website for specific requirements for admission. The program website is the official source of information for application requirements. Programs establish their own requirements and timeline for the application review. If the graduate program website is different from Rackham’s website, follow the graduate programs website instructions. Review the  page for program website and contact information.The graduate program will make a decision on your application and notify both you and Rackham. If you are recommended for admission, Rackham will confirm that you meet the minimum admission requirements and finalize the offer. Review the  for eligibility. Applicants are not admitted until they receive an email from Rackham with the offer of admission.Over 8,300 students are enrolled in Rackham degree programs taught and advised by faculty in graduate programs situated within 18 of the 19 schools and colleges across the Ann Arbor campus. Another 7,000 students are enrolled in graduate and professional degree programs administered separately by individual schools and colleges at U-M.Learn about the application review process and working with Rackham Graduate School and the graduate program.From creating an Applyweb account to submitting your letters of recommendation, see a step-by-step summary of our application process.Required Academic Credentials from Non-U.S. InstitutionsImmigration Process for International Students Transferring a Visa from Another U.S. Institution.To make the activity and culture of graduate programs more visible, we provide basic statistics about the master’s and Ph.D. programs at the university. The University of Michigan's commitment to diversity is a central part of our mission to ensure the excellence of graduate education.Why Study Public Health?The application for the Master of Science program in Biostatistics is administered by the Horace H. Rackham School of Graduate Studies. Applications for the Fall 2023 semester are no longer being accepted.sph-inquiries@umich.edu if you have questions.Applications are no longer being accepted for the Fall 2023 semester. Applications for the Fall 2024 semester will open in September of 2023. when submitting transcripts.Courses without grades or evaluation of class performance by the instructor typically will not satisfy program admissions requirements.The duration of this relaxation of departmental policy may be extended depending on the duration of the COVID-19 pandemic.How Do I Apply?Why Study Public Health?The MS and MPH programs in Biostatistics and the MS in Health Data Science are designed for completion in four terms (twenty months) with a total of 48 credits. See requirements for each of them below. The MPH requires an 8-week internship. How Do I Apply?Why Study Public Health?Graduate programs in the Department of Biostatistics at the University of Michigan School of Public Health are among the best in the world. Currently, we are ranked as the No. 4 biostatistics department by US News and World Report. Alongside our world-renowned faculty, you’ll learn how to develop statistical designs and computational tools for analysis of large scale complex data in medical and health sciences, and work to apply statistical methods to discover groundbreaking scientific findings.The Department of Biostatistics at the University of Michigan is excited to welcome to Ann Arbor its newest cohort of potential future Wolverines to the 2023 Biostatistics Admitted Student Experience (BASE). We are no longer accepting applications for the Fall 2023 semester. Applications for Fall 2024 will open in September 2023.Two-year, 48-credit-hour professional degree with an emphasis on practice experience. Twenty-month, 48-credit-hour degree for students who plan to go on to pursue a PhD or scientific research and academic careers, including jobs in biostatistics and data science. With the MS we also offer: We offer two MS degrees: MS in Biostatistics and MS in Health Data Science.High-level research track with original research as part of a doctoral dissertation. We offer two programs of study: one for students with a relevant master's degree and one for students without a relevant master's degree.We are driven by our collective mission to help people and our resolute passion for problem solving. We are innovators and collaborators; we are thinkers and we are doers.Our graduates have great job opportunities in academia, government, industry, and various other research institutions. Discover what you can accomplish with a degree from the Department of Biostatistics.With more than $50M in funded research annually, Biostatistics faculty and students are conducting cutting-edge biostatistical research. They are involved in a wide range of collaborative research activities with faculty across the University of Michigan campus. Among our research areas:Department of Biostatistics faculty and researchers have achieved national and international reputations for excellence in their field. They bring biostatistical design and analysis expertise to a wide spectrum of health-related issues.Start with our Applications & Deadlines to learn about specific degree application deadlines and requirements. International applicants will find additional information on our International Students page.sph-inquiries@umich.eduIf you would like to receive more information about a specific department or program, please join our interest list. You can also request a phone, zoom or in-person appointment.How Do I Apply? if you are not redirected within a few seconds.sph.umich.edu › biostat › programs › mastersThe MS and MPH programs in Biostatistics and the MS in Health Data Science are designed for completion in four terms (twenty months) with a total of 48 credits.sph.umich.edu › biostatsph.umich.edu › biostat › programs › masters-biostatIs a masters in biostatistics hard?Is MS biostatistics a stem?What is the acceptance rate for data science masters at umich?Is University of Michigan good for Masters?sph.umich.edu › biostat › apply-ms-biostatisticsThe application for the Master of Science program in Biostatistics is administered by the Horace H. Rackham School of Graduate Studies.sph.umich.edu › biostat › programs › masters-amdsph.umich.edu › biostat › programssph.umich.edu › biostat › programs › masters-hdssph.umich.edu › biostat › apply-mph-biostatsph.umich.edu › biostat › masters-student-directorysph.umich.edu › biostat › biostatistics-prospective-studentsMASTER OF SCIENCE in Biostatistics Degree · MASTER OF SCIENCE in health data science Degree · MASTER OF PUBLIC HEALTH Degree · PhD in Biostatistics.\n",
      "{'graduate': 48, 'graduates': 44, 'rackham': 42, 'scholarship': 41, 'doctoral': 39, 'studentsmaster': 39, 'admissions': 39, 'university': 38, 'phd': 38, 'enrolled': 38, 'campus': 37, 'edu': 36, 'degree': 35, 'academic': 34, 'masters': 34, 'educational': 33, 'academia': 33, 'courses': 33, 'education': 32, 'student': 32, 'applicants': 32, 'careers': 32, 'admission': 32, 'faculty': 32, 'dissertation': 31}\n",
      "{'graduate': 40.0, 'graduates': 31.764705882352942, 'rackham': 27.64705882352942, 'scholarship': 25.588235294117652, 'doctoral': 21.470588235294116, 'studentsmaster': 21.470588235294116, 'admissions': 21.470588235294116, 'university': 19.411764705882348, 'phd': 19.411764705882348, 'enrolled': 19.411764705882348, 'campus': 17.352941176470594, 'edu': 15.294117647058826, 'degree': 13.235294117647058, 'academic': 11.176470588235304, 'masters': 11.176470588235304, 'educational': 9.117647058823536, 'academia': 9.117647058823536, 'courses': 9.117647058823536, 'education': 7.058823529411768, 'student': 7.058823529411768, 'applicants': 7.058823529411768, 'careers': 7.058823529411768, 'admission': 7.058823529411768, 'faculty': 7.058823529411768, 'dissertation': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# n: number of keywords\n",
    "n = 25\n",
    "\n",
    "print(article)\n",
    "dic = keybert_extractor(article, n)\n",
    "print(dic)  # the output with original scores\n",
    "\n",
    "# convert scores to scale of 5 - 40\n",
    "scores = []\n",
    "result = {}\n",
    "\n",
    "# get every score, calculate scale ratio\n",
    "for keyword in dic.items():\n",
    "    scores.append(keyword[1])\n",
    "scores.sort()\n",
    "original_scale = float(scores[n-1] - scores[0])\n",
    "# print(original_scale)   # 17.0\n",
    "ratio = float(35.0 / original_scale)\n",
    "base = scores[0] * ratio - 5\n",
    "\n",
    "# loop through every score to update converted value\n",
    "for keyword in dic.items():\n",
    "    result[keyword[0]] = keyword[1] * ratio - base\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Serializing json\n",
    "json_object = json.dumps(result, indent=4)\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(\"../js/wordcloud//keywords.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
